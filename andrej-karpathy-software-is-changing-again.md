Last login: Thu Jul  3 12:25:35 on ttys025
source ~/.zshrc
j 
~/git/replicate/replicate-typescript-stainless/packages/mcp-server/cloudflare-worker add-instructions-for-chorus
$ j gemini
/Users/z/git/zeke/gemini-media-interpreter

~/git/zeke/gemini-media-interpreter main
$ gs
## main...origin/main

~/git/zeke/gemini-media-interpreter main
$ code .

~/git/zeke/gemini-media-interpreter main
$ python video.py "https://www.youtube.com/watch?v=LCEmiRjPEtQ"
Using Gemini model: gemini-2.5-flash-preview-05-20
Using YouTube media: https://www.youtube.com/watch?v=LCEmiRjPEtQ

Analyzing media...
Here's a summary of the video:

## Title
Software in the era of artificial intelligence.
Understanding the changing landscape of software in the age of AI.
Navigating the future of software development with AI.

## TLDR
Learn to build autonomous software using new AI paradigms.
Master programming new AI models with natural language.
Leverage LLMs to transform software development workflows.

## One-paragraph summary
This video teaches you how to understand and adapt to the rapidly evolving world of software in the era of AI. Learn about Software 1.0 (traditional code), Software 2.0 (neural network weights), and the emerging Software 3.0 (LLM prompts). Discover how LLMs are transforming software development, acting like new operating systems or utilities that can be programmed in natural language. Gain insights into building partially autonomous applications by understanding LLM psychology and leveraging new tools that enhance human-AI collaboration.

## TOC
00:00 Introduction to software in the era of AI
00:47 Software is changing (again)
01:36 Software 1.0 vs. Software 2.0
02:40 Introduction to Software 3.0 (LLM prompts)
03:32 Sentiment classification example: Software 1.0, 2.0, and 3.0
04:15 Tesla Autopilot: Software 2.0 eating Software 1.0
05:36 The future of software development
06:08 How to think about LLMs
06:23 LLMs as utilities
08:04 LLMs as fabs
09:15 LLMs as operating systems
10:37 Running apps on LLMs
11:00 LLMs and the time-sharing era of computing
11:48 Early hints of personal computing v2
12:10 Text chat as a terminal: The missing GUI
12:44 LLM psychology: Stochastic simulations of people
15:28 Encyclopedic knowledge and memory
16:09 Hallucinations and jagged intelligence
16:40 Anterograde amnesia
17:21 Popular culture and LLM limitations
17:39 Gullibility and prompt injection risks
18:00 Summary of LLM properties
18:18 Opportunities for LLMs
18:29 Partial autonomy apps
18:49 Anatomy of Cursor: An LLM-integrated IDE
20:59 Anatomy of Perplexity: An LLM research tool
21:28 What does software look like in a partial autonomy world?
22:08 The full workflow of partial autonomy UI/UX
23:33 Example: Keeping agents on the leash (AI-assisted coding)
24:55 Example: Keeping agents on the leash (Education)
25:49 Example: Tesla Autopilot and the autonomy slider
26:19 2015-2025: The decade of "driving agents"
27:27 Software is tricky: Human in the loop
28:23 Building autonomous software: Iron Man suits vs. robots
29:08 Make software highly accessible
29:40 Vibe coding
31:04 Vibe coding iOS app example
31:37 Vibe coding MenuGen app example
32:20 The code was the easiest part
33:37 Build for agents
34:10 Building context builders for LLMs
34:38 Docs for people and LLMs
35:09 Manim: Mathematical animation engine
35:56 Actions for people/LLMs
38:16 Conclusion and summary

## Transcript
Please welcome former director of AI Tesla, Andrej Karpathy. Woo. Wow, a lot of people here. Hello. Um, okay, yeah, so I'm excited to be here today to talk to you about software in the era of AI. And I'm told that many of you are students, like bachelors, masters, PhD and so on and you're about to enter the industry. And I think it's actually like an extremely unique and very interesting time to enter the industry right now. And I think fundamentally the reason for that is that, um, software is changing. Uh, again. And I say again, because I actually gave this talk already. Um, but the problem is that software keeps changing, so I actually have a lot of material to create new talks. And I think it's changing quite fundamentally. I think roughly speaking, software has not changed much on such a fundamental level for 70 years, and then it's changed, I think about twice quite rapidly in the last few years. And so there's just a huge amount of work to do, a huge amount of software to write and rewrite. So let's take a look at maybe the realm of software. So if we kinda think of this as like the map of software, this is a really cool tool called Map of GitHub. Um, this is kinda like all the software that's written. These are instructions to the computer for carrying out tasks in the digital space. So if you zoom in here, these are all different kinds of repositories, and this is all the code that has been written. And a few years ago, I kind of observed that, um, software was kind of changing and there was kind of like a new new type of software around. And I called this Software 2.0 at the time. And the idea here was that, uh, Software 1.0 is the code you write for the computer. Software 2.0 are basically neural networks, and in particular, the weights of a neural network. And you're not writing this code directly, you are, uh, you are more kinda like tuning the data sets and then you're running an optimizer to create to create the parameters of this neural net. And I think like, at the time neural network are kinda seen as like just a different kind of classifier, like a decision tree or something like that. And so I think, uh, it was kind of like, um, I I think this framing was a lot more appropriate. And now actually what we have is kind of like an equivalent of GitHub in the realm of Software 2.0. And I think the HuggingFace, uh, is basically equivalent of GitHub in Software 2.0. And there's also Model Atlas, and you can visualize all the code written there. In case you're curious, by the way, the giant circle, the point in the middle, uh, these are the parameters of Flux the image generator. And so anytime someone tunes a Lora on top of a Flux model, you basically create a Git commit, uh, in this space and, uh, you create a different kind of a image generator. So basically what we have is Software 1.0 is the computer code that programs a computer. Software 2.0 are the weights which program neural networks. Uh, and here's an example of AlexNet, image recognizer, neural network. Now, so far, all of the neural networks that we've been familiar with until recently, were kind of like fixed function computers. Image to categories, or something like that. And I think what's changed and I think as a quite fundamental change, is that neural networks became programmable with large language models. And so I I see this as quite new, unique, it's new kind of a computer and, uh, so in my mind, it's, uh, worth giving it a new designation of Software 3.0. And basically your prompts are now programs that program the LLM. And, uh, remarkably, uh, these, uh, prompts are written in English. So, it's kind of a very interesting programming language. Um, so maybe, uh, to, uh, summarize the difference. If you're doing sentiment classification, for example, you can imagine writing some, uh, amount of Python to to basically do sentiment classification, or you can train a neural net, or you can prompt a large language model. Uh, so here, I'm this is a few-shot prompt, and you can imagine changing it and programming the computer in a slightly different way. So, basically we have Software 1.0, Software 2.0, and I think we're seeing, I I maybe you've seen a lot of GitHub code is not just like code anymore. There's a bunch of like English interspersed with code. And so I think, kind of, there's a growing category of new kind of code. So not only is it a new programming paradigm, it's also remarkable to me that it's in our native language of English. And so when this blew my mind, uh, a few, uh, I guess years ago now, uh, I tweeted this and, um, I think it captured the attention of a lot of people. And this is my currently pinned tweet. Uh, is that remarkably, we're now programming computers in English. Now, when I was at, uh, Tesla, um, we were working on the, uh, Autopilot. And, uh, we were trying to get the car to drive. And I sort of showed this slide at the time, where you can imagine that the inputs to the car are on the bottom, and they're going through a software stack to produce the steering and acceleration. And I made the observation at the time that there was a ton of C++ code around in the Autopilot, which was the Software 1.0 code. And then there was some neural nets in there doing image recognition. And, uh, I kind of observed that over time, as we made the Autopilot better, basically, the neural network grew in capability and size. And in addition to that, all the C++ code was being deleted, and kind of like was, um, and a lot of the kind of capabilities and functionality that was originally written in 1.0 was migrated to 2.0. So, as an example, a lot of the stitching up of information across images from the different cameras and across time was done by a neural network, and we were able to delete a lot of code. And so the Software 2.0 stack would quite literally ate through the software stack of the Autopilot. So I thought this was really remarkable at the time. And I think we're seeing the same thing again. Where, uh, basically, we have a new kind of software, and it's eating through the stack. We have three completely different programming paradigms. And I think if you're entering the industry, it's a very good idea to be fluent in all of them, because they all have slight pros and cons, and you may want to program some functionality in 1.0 or 2.0 or 3.0. Are you gonna train your own net? Are you gonna just prompt an LLM? Uh, should this be a piece of code that's explicit, et cetera. So, we'll all have to make these decisions and actually potentially, uh, fluidly transition between these paradigms. So, what I wanted to get into now is, first, I want to, in the first part, talk about LLMs, and how to kind of like think of this new paradigm and the ecosystem, and what that looks like. Like, what are, what is this new computer? What does it look like? And what does the ecosystem look like? Um, I was struck by this quote from Andrew Ng actually, uh, many years ago now, I think. And I think Andrew's going to, uh, be speaking right after me. Uh, but he said at the time, AI is the new electricity. And I do think that it, um, kind of captures something very interesting in that LLMs certainly feel like they have properties of utilities right now. So, um, LLM labs, like OpenAI, Gemini, Anthropic, et cetera, they spend CAPEX to train the LLMs, and this is kind of equivalent to building out a grid. And then there's OPEX to serve that intelligence over APIs to all of us. And this is done through metered access, where we pay per million tokens or something like that. And we have a lot of demands that are very utility-like demands out of this API. We demand low latency, high uptime, consistent quality, et cetera. In electricity, you would have a transfer switch, so you can transfer your electricity source from, like, grid and solar or battery or generator. In LLMs, we have maybe OpenRouter and easily switch between the different types of LLMs that exist. Because the LLMs are software, they don't compete for physical space. So it's okay to have basically like six electricity providers, and you can switch between them, right? Uh, because they don't compete in such a direct way. And I think what's also a little fascinating, and we saw this in the last few days, actually, a lot of the LLMs went down, and people were kind of like stuck and unable to work. And I think it's kind of fascinating to me that when the state-of-the-art LLMs go down, it's actually kind of like an intelligence brownout in the world. It's kind of like when the voltage is unreliable in the grid. And, uh, the planet just gets dumber the more reliance we have on these models, which already is like really dramatic, and I think we'll continue to grow. But LLMs don't only have properties of utilities. I think it's also fair to say that they have some properties of fabs. And the reason for this is that the CAPEX required for building LLMs is actually quite large. Uh, it's not just like building some, uh, power station or something like that, right? Uh, you're investing a huge amount of money. And I think the tech tree and, uh, for the technology is growing quite rapidly. So we're in a world where we have sort of deep tech trees, research and development, secrets that are centralizing inside the LLM labs. Um, and I but I think the analogy muddies a little bit also, because as I mentioned, this is software, and software is a bit less defensible, uh, because it is so malleable. And so, um, I think it's just an interesting kind of thing to think about potentially. There's many analogies you can make. Like a four nanometer process node, maybe it's something like a cluster with certain max FLOPs. Uh, you can think about when you're using, when you're using NVIDIA GPUs, and you're only doing the software, and you're not doing the hardware, that's kind of like the fabless model. Uh, but if you're actually also building your own hardware, and you're training on TPUs, if you're Google, that's kind of like the Intel model where you own your fab. So I think there's some analogies here that make sense. But actually, I think the analogy that makes the most sense perhaps, is that in my mind, LLMs have very strong, kind of, analogies to operating systems. Uh, in that, this is not just electricity or water. It's not something that comes out of a tap as a commodity. Uh, this is these are now increasingly complex software ecosystems. Right? So, uh, they're not just like simple commodities like electricity. And it's kind of interesting to me that the ecosystem is shaping in a very similar kind of way where you have a few closed-source providers, like Windows or MacOS. And then you have an open-source alternative, like Linux. And I think for, uh, neural for LLMs as well, we have a kind of a few competing closed-source providers. And then maybe the Llama ecosystem is currently like, maybe a close approximation to something that may grow into something like Linux. Again, I think it's still very early because these are just simple LLMs, but we're starting to see that these are going to get a lot more complicated. It's not just about the LLM itself, it's about all the tool use and the multimodelities and how all of that works. And so when I sort of had this realization a while back, I tried to sketch it out. And it kind of seemed to me like LLMs are kind of like a new operating system, right? So, the LLM is a new kind of a computer. It's sitting, it's kind of like the CPU equivalent. Uh, the context windows are kind of like the memory. And then the LLM is orchestrating memory and compute, uh, for problem solving, um, using all of these capabilities here. And so, definitely, if you look at it, it looks very much like a software like operating system from that perspective. Um, a few more analogies. For example, if you want to download an app, say I go to VS Code, and I go to download, you can download VS Code, and you can run it on Windows, Linux, or or Mac. In the same way, as you can take an LLM app, like Cursor, and you can run it on GPT, or Claude, or Gemini series, right? There's just a dropdown. So, it's kind of like similar in that way as well. Uh, more analogies that I think strike me is that we're kind of like in this 1960s-ish era, where LLM compute is still very expensive for this new kind of a computer. And that forces the LLMs to be centralized in the cloud, and we're all just, uh, sort of thin clients that interact with it over the network. And none of us have full utilization of these computers, and therefore, it makes sense to use time-sharing, where we're all just, you know, uh, a dimension of the batch when they're running the computer in the cloud. And this is very much what computers used to look like at during this time. The operating systems were in the cloud, everything was streamed around, and there was batching. And so the the personal computing revolution hasn't happened yet because it's just not economical, it doesn't make sense, but I think some people are trying. And it turns out that Mac Minis, for example, are a very good fit for some of the LLMs, because it's all, if you're doing batch one inference, this is all super memory bound. So this actually works. And, uh, I think these are some early indications maybe of personal computing, but this hasn't really happened yet. It's not clear what this looks like. Maybe some of you get to invent what, what this is, or how it works, or, uh, what it should, what it should be. Maybe one more analogy that I'll mention is whenever I talk to ChatGPT, or some LLM directly in text, I feel like I'm talking to an operating system through the terminal. Like it just, it's, it's text, it's direct access to the operating system. And I think a GUI hasn't yet really been invented in like a general way. Like, should ChatGPT have a GUI, different than just the tech bubbles? Uh, certainly some of the apps that we're gonna go into in a bit, have GUI, but there's no like GUI across all the tasks that make sense. Um, there are some ways in which LLMs are different from kind of operating systems in some fairly unique way, and from early computing. And I wrote about, uh, this, uh, one particular property that strikes me as very different, uh, this time around. It's that LLMs like flip, they flip the direction of technology diffusion, uh, that is usually, uh, present in technology. So, for example, with electricity, cryptography, computing, flight, internet, GPS, lots of new transformative technologies that have not been around. Typically, it is the government and corporations that are the first users, because it's new and expensive, et cetera. And it only later diffuses to consumer. Uh, but I feel like LLMs are kind of like flipped around. So maybe with early computers, it was all about ballistics and military use, but with LLMs, it's all about how do you boil an egg or something like that. This is certainly a lot of my use. So, it's really fascinating to me that we have a new magical computer, and it's like helping me boil an egg. It's not helping the government do something really crazy like some military ballistics or some special technology. Indeed, corporations and governments are lagging behind the adoption of all of us, of all of these technologies. So, it's just backwards. And I think it informs maybe some of the uses of how we want to use this technology, or like, what are some of the first apps and so on. So, in summary so far, LLM labs, fab LLMs, I think, it's accurate language to use. But LLMs are complicated operating systems. They're circa 1960s in computing, and we're redoing computing all over again. And they're currently available via time-sharing and distributed like a utility. What is new and unprecedented, is that they're not in the hands of a few governments and corporations. They're in the hands of all of us, because we all have a computer, and it's all just software, and ChatGPT was beamed down to our computers like to billions of people like instantly and overnight. And this is insane. Uh, and it's kind of insane to me that this is the case, and now it is our time to enter the industry and program these computers. This is crazy. So, I think this is quite remarkable. Before we program LLMs, we have to kind of like spend some time to think about what these things are, and I especially like to kind of talk about their psychology. So, the way I like to think about LLMs is that they're kind of like people spirits. Um, they are stochastic simulations of people. Um, and the simulator in this case happens to be an autoregressive Transformer. So, Transformer is a neural net. Uh, it's and it just kind of like goes on the level of tokens, it goes chunk, chunk, chunk, chunk. And there's an almost equal amount of compute for every single chunk. Um, and, um, this simulator, of course, is is just, is basically there's some weights involved, and we fit it to all of text that we have on the internet and so on. And you end up with this kind of a simulator. And because it is trained on humans, it's got this emergent psychology that is human-like. So the first thing you'll notice is, of course, LLMs have encyclopedic knowledge and memory. Uh, and they can remember lots of things, a lot more than any single individual human can, because they have read so many things. It's it actually kind of reminds me of this movie Rain Man, which I actually really recommend people watch. It's an amazing movie, I love this movie. Um, and Dustin Hoffman here is an autistic savant, who has almost perfect memory. So he can read it a, he can read like a phone book, and remember all of the names and, uh, phone numbers. And I kind of feel like LLMs are kind of like very similar. They can remember SHA hashes and lots of different kinds of things very, very easily. So, they certainly have superpowers in some set, in some respects, but they also have a bunch of, I would say, cognitive deficits. So, they hallucinate quite a bit. Um, and they kind of make up stuff and don't have a very good, uh, sort of internal model of self-knowledge, uh, not sufficient, at least. And this has gotten better, but not perfect. They display jagged intelligence. So, they're going to be superhuman in some problem-solving domains. And then they're gonna make mistakes that basically no human will make. Like, you know, they will insist that 9.11 is greater than 9.9, or that there are two Rs in strawberry. These are some famous examples. But basically, there are rough edges that you can trip on. So, that's kind of, I think, also kind of unique. Um, they also kind of suffer from anterograde amnesia. Um, so, uh, and I think I'm alluding to the fact that if you have a coworker who joins your organization, this coworker will over time, learn your organization, and, uh, they will understand and gain like a huge amount of context on the organization. And they go home, and they sleep, and they consolidate knowledge, and they develop expertise over time. LLMs don't natively do this. And this is not something that has really been solved in the R&D of LLMs, I think. Um, and so context windows are really kind of like working memory. And you have to sort of program the working memory quite directly because they don't just kind of like get smarter by, uh, by default. And I think a lot of people get tripped up by the analogies, um, in this way. In popular culture, I recommend people watch these two movies, um, Memento and 50 First Dates. In both of these movies, the protagonists, their weights are fixed, and their context windows gets wiped every single morning. And it's really problematic to go to work or have relationships when this happens, and this happens to LLMs all the time. I guess one more thing I would point to is security, kind of, related limitations of the use of LLMs. So, for example, LLMs are quite gullible. Uh, they are susceptible to prompt injection risks. They might leak your data, et cetera. And so, um, and there's many other considerations, security-related. So, so, basically, long story short, you have to load your, you have to load your, this is simultaneously think through this superhuman thing that has a bunch of cognitive deficits and issues. How do we and yet, they are extremely, like, useful. And so, how do we program them, and how do we work around their deficits, and enjoy their superhuman powers. So, what I want to switch to now is, first, I want to, in the first part, talk about LLMs, and how to kind of like think of this new paradigm and the ecosystem, and what that looks like. Like, what are, what is this new computer? What does it look like? And what does the ecosystem look like? Um, I was struck by this quote from Andrew Ng actually, uh, many years ago now, I think. And I think Andrew's going to, uh, be speaking right after me. Uh, but he said at the time, AI is the new electricity. And I do think that it, um, kind of captures something very interesting in that LLMs certainly feel like they have properties of utilities right now. So, um, LLM labs, like OpenAI, Gemini, Anthropic, et cetera, they spend CAPEX to train the LLMs, and this is kind of equivalent to building out a grid. And then there's OPEX to serve that intelligence over APIs to all of us. And this is done through metered access, where we pay per million tokens or something like that. And we have a lot of demands that are very utility-like demands out of this API. We demand low latency, high uptime, consistent quality, et cetera. In electricity, you would have a transfer switch, so you can transfer your electricity source from, like, grid and solar or battery or generator. In LLMs, we have maybe OpenRouter and easily switch between the different types of LLMs that exist. Because the LLMs are software, they don't compete for physical space. So it's okay to have basically like six electricity providers, and you can switch between them, right? Uh, because they don't compete in such a direct way. And I think what's also a little fascinating, and we saw this in the last few days, actually, a lot of the LLMs went down, and people were kind of like stuck and unable to work. And I think it's kind of fascinating to me that when the state-of-the-art LLMs go down, it's actually kind of like an intelligence brownout in the world. It's kind of like when the voltage is unreliable in the grid. And, uh, the planet just gets dumber the more reliance we have on these models, which already is like really dramatic, and I think we'll continue to grow. But LLMs don't only have properties of utilities. I think it's also fair to say that they have some properties of fabs. And the reason for this is that the CAPEX required for building LLMs is actually quite large. Uh, it's not just like building some, uh, power station or something like that, right? Uh, you're investing a huge amount of money. And I think the tech tree and, uh, for the technology is growing quite rapidly. So we're in a world where we have sort of deep tech trees, research and development, secrets that are centralizing inside the LLM labs. Um, and I but I think the analogy muddies a little bit also, because as I mentioned, this is software, and software is a bit less defensible, uh, because it is so malleable. And so, um, I think it's just an interesting kind of thing to think about potentially. There's many analogies you can make. Like a four nanometer process node, maybe it's something like a cluster with certain max FLOPs. Uh, you can think about when you're using, when you're using NVIDIA GPUs, and you're only doing the software, and you're not doing the hardware, that's kind of like the fabless model. Uh, but if you're actually also building your own hardware, and you're training on TPUs, if you're Google, that's kind of like the Intel model where you own your fab. So I think there's some analogies here that make sense. But actually, I think the analogy that makes the most sense perhaps, is that in my mind, LLMs have very strong, kind of, analogies to operating systems. Uh, in that, this is not just electricity or water. It's not something that comes out of a tap as a commodity. Uh, this is these are now increasingly complex software ecosystems. Right? So, uh, they're not just like simple commodities like electricity. And it's kind of interesting to me that the ecosystem is shaping in a very similar kind of way where you have a few closed-source providers, like Windows or MacOS. And then you have an open-source alternative, like Linux. And I think for, uh, neural for LLMs as well, we have a kind of a few competing closed-source providers. And then maybe the Llama ecosystem is currently like, maybe a close approximation to something that may grow into something like Linux. Again, I think it's still very early because these are just simple LLMs, but we're starting to see that these are going to get a lot more complicated. It's not just about the LLM itself, it's about all the tool use and the multimodelities and how all of that works. And so when I sort of had this realization a while back, I tried to sketch it out. And it kind of seemed to me like LLMs are kind of like a new operating system, right? So, the LLM is a new kind of a computer. It's sitting, it's kind of like the CPU equivalent. Uh, the context windows are kind of like the memory. And then the LLM is orchestrating memory and compute, uh, for problem solving, um, using all of these capabilities here. And so, definitely, if you look at it, it looks very much like a software like operating system from that perspective. Um, a few more analogies. For example, if you want to download an app, say I go to VS Code, and I go to download, you can download VS Code, and you can run it on Windows, Linux, or or Mac. In the same way, as you can take an LLM app, like Cursor, and you can run it on GPT, or Claude, or Gemini series, right? There's just a dropdown. So, it's kind of like similar in that way as well. Uh, more analogies that I think strike me is that we're kind of like in this 1960s-ish era, where LLM compute is still very expensive for this new kind of a computer. And that forces the LLMs to be centralized in the cloud, and we're all just, uh, sort of thin clients that interact with it over the network. And none of us have full utilization of these computers, and therefore, it makes sense to use time-sharing, where we're all just, you know, uh, a dimension of the batch when they're running the computer in the cloud. And this is very much what computers used to look like at during this time. The operating systems were in the cloud, everything was streamed around, and there was batching. And so the the personal computing revolution hasn't happened yet because it's just not economical, it doesn't make sense, but I think some people are trying. And it turns out that Mac Minis, for example, are a very good fit for some of the LLMs, because it's all, if you're doing batch one inference, this is all super memory bound. So this actually works. And, uh, I think these are some early indications maybe of personal computing, but this hasn't really happened yet. It's not clear what this looks like. Maybe some of you get to invent what, what this is, or how it works, or, uh, what it should, what it should be. Maybe one more analogy that I'll mention is whenever I talk to ChatGPT, or some LLM directly in text, I feel like I'm talking to an operating system through the terminal. Like it just, it's, it's text, it's direct access to the operating system. And I think a GUI hasn't yet really been invented in like a general way. Like, should ChatGPT have a GUI, different than just the tech bubbles? Uh, certainly some of the apps that we're gonna go into in a bit, have GUI, but there's no like GUI across all the tasks that make sense. Um, there are some ways in which LLMs are different from kind of operating systems in some fairly unique way, and from early computing. And I wrote about, uh, this, uh, one particular property that strikes me as very different, uh, this time around. It's that LLMs like flip, they flip the direction of technology diffusion, uh, that is usually, uh, present in technology. So, for example, with electricity, cryptography, computing, flight, internet, GPS, lots of new transformative technologies that have not been around. Typically, it is the government and corporations that are the first users, because it's new and expensive, et cetera. And it only later diffuses to consumer. Uh, but I feel like LLMs are kind of like flipped around. So maybe with early computers, it was all about ballistics and military use, but with LLMs, it's all about how do you boil an egg or something like that. This is certainly a lot of my use. So, it's really fascinating to me that we have a new magical computer, and it's like helping me boil an egg. It's not helping the government do something really crazy like some military ballistics or some special technology. Indeed, corporations and governments are lagging behind the adoption of all of us, of all of these technologies. So, it's just backwards. And I think it informs maybe some of the uses of how we want to use this technology, or like, what are some of the first apps and so on. So, in summary so far, LLM labs, fab LLMs, I think, it's accurate language to use. But LLMs are complicated operating systems. They're circa 1960s in computing, and we're redoing computing all over again. And they're currently available via time-sharing and distributed like a utility. What is new and unprecedented, is that they're not in the hands of a few governments and corporations. They're in the hands of all of us, because we all have a computer, and it's all just software, and ChatGPT was beamed down to our computers like to billions of people like instantly and overnight. And this is insane. Uh, and it's kind of insane to me that this is the case, and now it is our time to enter the industry and program these computers. This is crazy. So, I think this is quite remarkable. Before we program LLMs, we have to kind of like spend some time to think about what these things are, and I especially like to kind of talk about their psychology. So, the way I like to think about LLMs is that they're kind of like people spirits. Um, they are stochastic simulations of people. Um, and the simulator in this case happens to be an autoregressive Transformer. So, Transformer is a neural net. Uh, it's and it just kind of like goes on the level of tokens, it goes chunk, chunk, chunk, chunk. And there's an almost equal amount of compute for every single chunk. Um, and, um, this simulator, of course, is is just, is basically there's some weights involved, and we fit it to all of text that we have on the internet and so on. And you end up with this kind of a simulator. And because it is trained on humans, it's got this emergent psychology that is human-like. So the first thing you'll notice is, of course, LLMs have encyclopedic knowledge and memory. Uh, and they can remember lots of things, a lot more than any single individual human can, because they have read so many things. It's it actually kind of reminds me of this movie Rain Man, which I actually really recommend people watch. It's an amazing movie, I love this movie. Um, and Dustin Hoffman here is an autistic savant, who has almost perfect memory. So he can read it a, he can read like a phone book, and remember all of the names and, uh, phone numbers. And I kind of feel like LLMs are kind of like very similar. They can remember SHA hashes and lots of different kinds of things very, very easily. So, they certainly have superpowers in some set, in some respects, but they also have a bunch of, I would say, cognitive deficits. So, they hallucinate quite a bit. Um, and they kind of make up stuff and don't have a very good, uh, sort of internal model of self-knowledge, uh, not sufficient, at least. And this has gotten better, but not perfect. They display jagged intelligence. So, they're going to be superhuman in some problem-solving domains. And then they're gonna make mistakes that basically no human will make. Like, you know, they will insist that 9.11 is greater than 9.9, or that there are two Rs in strawberry. These are some famous examples. But basically, there are rough edges that you can trip on. So, that's kind of, I think, also kind of unique. Um, they also kind of suffer from anterograde amnesia. Um, so, uh, and I think I'm alluding to the fact that if you have a coworker who joins your organization, this coworker will over time, learn your organization, and, uh, they will understand and gain like a huge amount of context on the organization. And they go home, and they sleep, and they consolidate knowledge, and they develop expertise over time. LLMs don't natively do this. And this is not something that has really been solved in the R&D of LLMs, I think. Um, and so context windows are really kind of like working memory. And you have to sort of program the working memory quite directly because they don't just kind of like get smarter by, uh, by default. And I think a lot of people get tripped up by the analogies, um, in this way. In popular culture, I recommend people watch these two movies, um, Memento and 50 First Dates. In both of these movies, the protagonists, their weights are fixed, and their context windows gets wiped every single morning. And it's really problematic to go to work or have relationships when this happens, and this happens to LLMs all the time. I guess one more thing I would point to is security, kind of, related limitations of the use of LLMs. So, for example, LLMs are quite gullible. Uh, they are susceptible to prompt injection risks. They might leak your data, et cetera. And so, um, and there's many other considerations, security-related. So, so, basically, long story short, you have to load your, you have to load your, this is simultaneously think through this superhuman thing that has a bunch of cognitive deficits and issues. How do we and yet, they are extremely, like, useful. And so, how do we program them, and how do we work around their deficits, and enjoy their superhuman powers. So, what I want to switch to now is, first, I want to, in the first part, talk about LLMs, and how to kind of like think of this new paradigm and the ecosystem, and what that looks like. Like, what are, what is this new computer? What does it look like? And what does the ecosystem look like? Um, I was struck by this quote from Andrew Ng actually, uh, many years ago now, I think. And I think Andrew's going to, uh, be speaking right after me. Uh, but he said at the time, AI is the new electricity. And I do think that it, um, kind of captures something very interesting in that LLMs certainly feel like they have properties of utilities right now. So, um, LLM labs, like OpenAI, Gemini, Anthropic, et cetera, they spend CAPEX to train the LLMs, and this is kind of equivalent to building out a grid. And then there's OPEX to serve that intelligence over APIs to all of us. And this is done through metered access, where we pay per million tokens or something like that. And we have a lot of demands that are very utility-like demands out of this API. We demand low latency, high uptime, consistent quality, et cetera. In electricity, you would have a transfer switch, so you can transfer your electricity source from, like, grid and solar or battery or generator. In LLMs, we have maybe OpenRouter and easily switch between the different types of LLMs that exist. Because the LLMs are software, they don't compete for physical space. So it's okay to have basically like six electricity providers, and you can switch between them, right? Uh, because they don't compete in such a direct way. And I think what's also a little fascinating, and we saw this in the last few days, actually, a lot of the LLMs went down, and people were kind of like stuck and unable to work. And I think it's kind of fascinating to me that when the state-of-the-art LLMs go down, it's actually kind of like an intelligence brownout in the world. It's kind of like when the voltage is unreliable in the grid. And, uh, the planet just gets dumber the more reliance we have on these models, which already is like really dramatic, and I think we'll continue to grow. But LLMs don't only have properties of utilities. I think it's also fair to say that they have some properties of fabs. And the reason for this is that the CAPEX required for building LLMs is actually quite large. Uh, it's not just like building some, uh, power station or something like that, right? Uh, you're investing a huge amount of money. And I think the tech tree and, uh, for the technology is growing quite rapidly. So we're in a world where we have sort of deep tech trees, research and development, secrets that are centralizing inside the LLM labs. Um, and I but I think the analogy muddies a little bit also, because as I mentioned, this is software, and software is a bit less defensible, uh, because it is so malleable. And so, um, I think it's just an interesting kind of thing to think about potentially. There's many analogies you can make. Like a four nanometer process node, maybe it's something like a cluster with certain max FLOPs. Uh, you can think about when you're using, when you're using NVIDIA GPUs, and you're only doing the software, and you're not doing the hardware, that's kind of like the fabless model. Uh, but if you're actually also building your own hardware, and you're training on TPUs, if you're Google, that's kind of like the Intel model where you own your fab. So I think there's some analogies here that make sense. But actually, I think the analogy that makes the most sense perhaps, is that in my mind, LLMs have very strong, kind of, analogies to operating systems. Uh, in that, this is not just electricity or water. It's not something that comes out of a tap as a commodity. Uh, this is these are now increasingly complex software ecosystems. Right? So, uh, they're not just like simple commodities like electricity. And it's kind of interesting to me that the ecosystem is shaping in a very similar kind of way where you have a few closed-source providers, like Windows or MacOS. And then you have an open-source alternative, like Linux. And I think for, uh, neural for LLMs as well, we have a kind of a few competing closed-source providers. And then maybe the Llama ecosystem is currently like, maybe a close approximation to something that may grow into something that, that may grow into something like Linux. Again, I think it's still very early because these are just simple LLMs, but we're starting to see that these are going to get a lot more complicated. It's not just about the LLM itself, it's about all the tool use and the multimodelities and how all of that works. And so when I sort of had this realization a while back, I tried to sketch it out. And it kind of seemed to me like LLMs are kind of like a new operating system, right? So, the LLM is a new kind of a computer. It's sitting, it's kind of like the CPU equivalent. Uh, the context windows are kind of like the memory. And then the LLM is orchestrating memory and compute, uh, for problem solving, um, using all of these capabilities here. And so, definitely, if you look at it, it looks very much like a software like operating system from that perspective. Um, a few more analogies. For example, if you want to download an app, say I go to VS Code, and I go to download, you can download VS Code, and you can run it on Windows, Linux, or or Mac. In the same way, as you can take an LLM app, like Cursor, and you can run it on GPT, or Claude, or Gemini series, right? There's just a dropdown. So, it's kind of like similar in that way as well. Uh, more analogies that I think strike me is that we're kind of like in this 1960s-ish era, where LLM compute is still very expensive for this new kind of a computer. And that forces the LLMs to be centralized in the cloud, and we're all just, uh, sort of thin clients that interact with it over the network. And none of us have full utilization of these computers, and therefore, it makes sense to use time-sharing, where we're all just, you know, uh, a dimension of the batch when they're running the computer in the cloud. And this is very much what computers used to look like at during this time. The operating systems were in the cloud, everything was streamed around, and there was batching. And so the the personal computing revolution hasn't happened yet because it's just not economical, it doesn't make sense, but I think some people are trying. And it turns out that Mac Minis, for example, are a very good fit for some of the LLMs, because it's all, if you're doing batch one inference, this is all super memory bound. So this actually works. And, uh, I think these are some early indications maybe of personal computing, but this hasn't really happened yet. It's not clear what this looks like. Maybe some of you get to invent what, what this is, or how it works, or, uh, what it should, what it should be. Maybe one more analogy that I'll mention is whenever I talk to ChatGPT, or some LLM directly in text, I feel like I'm talking to an operating system through the terminal. Like it just, it's, it's text, it's direct access to the operating system. And I think a GUI hasn't yet really been invented in like a general way. Like, should ChatGPT have a GUI, different than just the tech bubbles? Uh, certainly some of the apps that we're gonna go into in a bit, have GUI, but there's no like GUI across all the tasks that make sense. Um, there are some ways in which LLMs are different from kind of operating systems in some fairly unique way, and from early computing. And I wrote about, uh, this, uh, one particular property that strikes me as very different, uh, this time around. It's that LLMs like flip, they flip the direction of technology diffusion, uh, that is usually, uh, present in technology. So, for example, with electricity, cryptography, computing, flight, internet, GPS, lots of new transformative technologies that have not been around. Typically, it is the government and corporations that are the first users, because it's new and expensive, et cetera. And it only later diffuses to consumer. Uh, but I feel like LLMs are kind of like flipped around. So maybe with early computers, it was all about ballistics and military use, but with LLMs, it's all about how do you boil an egg or something like that. This is certainly a lot of my use. So, it's really fascinating to me that we have a new magical computer, and it's like helping me boil an egg. It's not helping the government do something really crazy like some military ballistics or some special technology. Indeed, corporations and governments are lagging behind the adoption of all of us, of all of these technologies. So, it's just backwards. And I think it informs maybe some of the uses of how we want to use this technology, or like, what are some of the first apps and so on. So, in summary so far, LLM labs, fab LLMs, I think, it's accurate language to use. But LLMs are complicated operating systems. They're circa 1960s in computing, and we're redoing computing all over again. And they're currently available via time-sharing and distributed like a utility. What is new and unprecedented, is that they're not in the hands of a few governments and corporations. They're in the hands of all of us, because we all have a computer, and it's all just software, and ChatGPT was beamed down to our computers like to billions of people like instantly and overnight. And this is insane. Uh, and it's kind of insane to me that this is the case, and now it is our time to enter the industry and program these computers. This is crazy. So, I think this is quite remarkable. Before we program LLMs, we have to kind of like spend some time to think about what these things are, and I especially like to kind of talk about their psychology. So, the way I like to think about LLMs is that they're kind of like people spirits. Um, they are stochastic simulations of people. Um, and the simulator in this case happens to be an autoregressive Transformer. So, Transformer is a neural net. Uh, it's and it just kind of like goes on the level of tokens, it goes chunk, chunk, chunk, chunk. And there's an almost equal amount of compute for every single chunk. Um, and, um, this simulator, of course, is is just, is basically there's some weights involved, and we fit it to all of text that we have on the internet and so on. And you end up with this kind of a simulator. And because it is trained on humans, it's got this emergent psychology that is human-like. So the first thing you'll notice is, of course, LLMs have encyclopedic knowledge and memory. Uh, and they can remember lots of things, a lot more than any single individual human can, because they have read so many things. It's it actually kind of reminds me of this movie Rain Man, which I actually really recommend people watch. It's an amazing movie, I love this movie. Um, and Dustin Hoffman here is an autistic savant, who has almost perfect memory. So he can read it a, he can read like a phone book, and remember all of the names and, uh, phone numbers. And I kind of feel like LLMs are kind of like very similar. They can remember SHA hashes and lots of different kinds of things very, very easily. So, they certainly have superpowers in some set, in some respects, but they also have a bunch of, I would say, cognitive deficits. So, they hallucinate quite a bit. Um, and they kind of make up stuff and don't have a very good, uh, sort of internal model of self-knowledge, uh, not sufficient, at least. And this has gotten better, but not perfect. They display jagged intelligence. So, they're going to be superhuman in some problem-solving domains. And then they're gonna make mistakes that basically no human will make. Like, you know, they will insist that 9.11 is greater than 9.9, or that there are two Rs in strawberry. These are some famous examples. But basically, there are rough edges that you can trip on. So, that's kind of, I think, also kind of unique. Um, they also kind of suffer from anterograde amnesia. Um, so, uh, and I think I'm alluding to the fact that if you have a coworker who joins your organization, this coworker will over time, learn your organization, and, uh, they will understand and gain like a huge amount of context on the organization. And they go home, and they sleep, and they consolidate knowledge, and they develop expertise over time. LLMs don't natively do this. And this is not something that has really been solved in the R&D of LLMs, I think. Um, and so context windows are really kind of like working memory. And you have to sort of program the working memory quite directly because they don't just kind of like get smarter by, uh, by default. And I think a lot of people get tripped up by the analogies, um, in this way. In popular culture, I recommend people watch these two movies, um, Memento and 50 First Dates. In both of these movies, the protagonists, their weights are fixed, and their context windows gets wiped every single morning. And it's really problematic to go to work or have relationships when this happens, and this happens to LLMs all the time. I guess one more thing I would point to is security, kind of, related limitations of the use of LLMs. So, for example, LLMs are quite gullible. Uh, they are susceptible to prompt injection risks. They might leak your data, et cetera. And so, um, and there's many other considerations, security-related. So, so, basically, long story short, you have to load your, you have to load your, this is simultaneously think through this superhuman thing that has a bunch of cognitive deficits and issues. How do we and yet, they are extremely, like, useful. And so, how do we program them, and how do we work around their deficits, and enjoy their superhuman powers. So, what I want to switch to now is to talk about the opportunities of how do we use these models and what are some of the biggest opportunities. This is not a comprehensive list, just some of the things that I thought were interesting for this talk. The first thing I'm kind of excited about is what I would call partial autonomy apps. So, for example, let's work with the example of coding. You can certainly go to ChatGPT directly and you can start copy-pasting code around and copy-pasting bug reports and stuff around and getting code and copy-pasting everything around. Why would you, why would you do that? Why would you go directly to the operating system? It makes a lot more sense to have an app dedicated for this. And so, I think many of you, uh, use Cursor. I do as well. Uh, and, uh, Cursor is kind of like the thing you want instead. You don't want to just directly go to the ChatGPT. And I think Cursor is a very good example of an early LLM app that has a bunch of properties that I think are, um, useful across all the LLM apps. So, in particular, you will notice that we have a traditional interface that allows a human to go in and do all the work manually, just as before. But in addition to that, we now have this LLM integration that allows us to go in bigger chunks. And so, some of the properties of LLM apps that I think are shared and useful to point out. Number one, the LLMs basically do a ton of the context management. Um, number two, they orchestrate multiple calls to LLMs, right? So, in the case of Cursor, there's under the hood, embedding models for all your files, the actual chat models, models that apply diffs to the code, and this is all orchestrated for you. A really big one that, uh, I think also maybe not fully appreciated always, is application-specific GUI, and the importance of it. Um, because you don't want to just talk to the operating system directly in text. Text is very hard to read, interpret, understand. And also, like, you don't want to take some of these actions natively in text. So, it's much better to just see a diff as like red and green change. And you can see what's being added and subtracted. It's much easier to just do Command Y to accept, or Command N to reject. I shouldn't have to type it in text, right? So, a GUI allows a human to audit the work of these fallible systems and to go faster. I'm going to come back to this point a little bit, uh, later as well. And the last kind of feature I want to point out is that there's what I call the autonomy slider. So, for example, in Cursor, you can just do tab completion. You're mostly in charge. You can select a chunk of code and Command K to change just that chunk of code. You can do Command L to change the entire file. Or you can do Command I, which just, you know, let 'er rip, do whatever you want in the entire repo. And that's the sort of full autonomy agentic version. And so, you are in charge of the autonomy slider. And depending on the complexity of the task at hand, you can, uh, tune the amount of autonomy that you're willing to give up for that task. Maybe to show one more example of a fairly successful LLM app, Perplexity. Um, they it also has very similar features to what I've just pointed out in Cursor. Uh, it packages up a lot of the information, it orchestrates multiple LLMs. It's got a GUI that allows you to audit some of its work. So, for example, it will, uh, cite sources, and you can imagine inspecting them. And it's got an autonomy slider. You can either just do a quick search, or you can do research, or you can do deep research, and come back 10 minutes later. So, this is all just varying levels of autonomy that you give up to the tool. So, I guess my question is, I feel like a lot of software will become partially autonomous. I'm trying to think through, like, what does that look like? And for many of you who maintain products and services, how are you going to make your products and services partially autonomous? Can an LLM see all the things the human can see? Can an LLM act in all the ways that a human can act? And can humans supervise and stay in the loop of this activity? Because again, these are fallible systems that aren't yet perfect. And what does a diff look like in Photoshop or something like that, you know? And also a lot of the traditional software right now, it has all these switches and all this kind of stuff that's all designed for human. All this has to change and become accessible to LLMs. So, one thing I want to stress with a lot of these LLM apps, that I'm not sure gets, uh, as much attention as it should, is, um, we're now kind of like cooperating with AIs. And usually, they are doing the generation, and we as humans are doing the verification. It is in our interest to make this loop go as fast as possible, so we're getting a lot of work done. There are two major ways that I, uh, think, uh, this can be done. Number one, you can speed up verification a lot. Um, and I think GUIs, for example, are extremely important to this. Because a GUI utilizes your computer vision GPU in all of our head. Reading text is effortful, and it's not fun, but looking at stuff is fun. And it's just a kind of like a highway to your brain. So, I think GUIs are very useful for auditing systems and visual representations in general. And number two, I would say, is, we have to keep the AI on the leash. We, I think a lot of people are getting way over-excited with AI agents, and, uh, it's not useful to me to get a diff of 1,000 lines of code to my repo. Like, I have to, I'm still the bottleneck, right? Even though the 1,000 lines come out instantly, I have to make sure that this thing is not introducing bugs, is just like, and that is doing the correct thing, right? And that there's no security issues, and so on. So, I'm, I think that, um, yeah, basically, you, we have to sort of, like, it's in our interest to make the, the flow of these two go very, very fast. And we have to somehow keep the AI on the leash because it gets way too over-reactive. It's, uh, it's kind of like this. This is how I feel when I do AI-assisted coding. If I'm just vibe-coding, everything is nice and great, but if I'm actually trying to get work done, it's not so great to have an over-reactive, uh, agents doing all this kind of stuff. So, this slide is not very good. I'm sorry, but, I guess I'm trying to develop, like many of you, some ways of utilizing these agents in my coding workflow, and to do AI-assisted coding. And in my own work, I'm always scared to get way too big diffs. I always go in small, incremental chunks. I want to make sure that everything is good. I want to spin this loop very, very fast. And, uh, I sort of work on small chunks of single concrete thing. Uh, and so, I think, uh, many of you probably are developing similar ways of working with the, with LLMs. Um, I also saw a number of blog posts that try to develop these best practices for working with LLMs. And here's one that I read recently, and I thought was quite good. And it kind of discusses some techniques. And some of them have to do with how you keep the AI on the leash. So, as an example, if you are prompting, if your prompt is big, then, uh, the AI might not do exactly what you wanted. And in that case, verification will fail. You're going to ask for something else. If a verification fails, then you're going to start spinning. So, it makes a lot more sense to spend a bit more time to be more concrete in your prompts, which increases the probability of successful verification, and you can move forward. And so, I think a lot of us are going to end up finding, um, kind of techniques like this. I think in my own work as well, I'm currently interested in, uh, what education looks like in, um, together with, kind of, like, now that we have AI, uh, and LLMs, what does education look like? And I think a lot, a large amount of thought for me goes into how we keep AI on the leash. I don't think it just works to go to ChatGPT and be like, hey, teach me physics. I don't think this works because the AI is like gets lost in the woods. And so, for me, this is actually two separate apps, for example. There's an app for a teacher that creates courses, and then there's an app that takes courses and serves them to students. And in both cases, we now have it this intermediate artifact, of a course that is auditable, and we can make sure it's good. We can make sure it's consistent. And the AI is kept on the leash with respect to a certain syllabus, a certain, like, um, progression of projects, and so on. And so, this is one way of keeping the AI on the leash. And I think it has a much higher likelihood of working, and the AI is not getting lost in the woods. One more kind of analogy I wanted to sort of allude to is, I'm not, I'm no stranger to partial autonomy, and I kind of worked on this, I think, for five years at Tesla. And this is also a partial autonomy product, and shares a lot of the features. Like, for example, right there in the instrument panel is the GUI of the Autopilot. So, it's showing me what the what the neural network sees, and so on. And we have the autonomy slider where over the course of my tenure there, we did more and more autonomous tasks for the user. And maybe the story that I wanted to tell very briefly is, uh, actually, the first time I drove a self-driving vehicle was in 2013. And I had a friend who worked at Waymo, and, uh, he offered to give me a drive around Palo Alto. I took this picture using Google Glass at the time. And many of you are so young that you might not even know what that is. Uh, but, uh, yeah, this was like all the rage at the time. And we got into this car, and we went for about a 30-minute drive around Palo Alto. Highways, uh, streets, and so on. And this drive was perfect. There were zero interventions. And this was 2013, which is now 12 years ago. And it's kind of struck me, because at the time when I had this perfect drive, this perfect demo, I felt like, wow, self-driving is imminent, because this just worked. This is incredible. Um, but here we are 12 years later, and we are still working on autonomy. Um, we are still working on driving agents. And even now, we haven't actually, like, fully solved the problem. Like, you may see Waymos going around, and they look driverless. But, you know, there's still a lot of tele-operation, and a lot of human in the loop of a lot of this driving. So, we still haven't even like declared success, but I think it's definitely like going to succeed at this point. But it just took a long time. And so, I think like, like, this is software is really tricky. I think in the same way that driving is tricky. And so, when I see things like, oh, 2025 is the year of agents, I get very concerned. And I kind of feel like, you know, this is the decade of agents. And this is going to be quite some time. We need humans in the loop. We need to do this carefully. This is software. Let's be serious here. One more, kind of, analogy that I always think through is the Iron Man suit. Uh, I think this is, I always love Iron Man. I think it's like so, um, correct in a bunch of ways with respect to technology, and how it will play out. And what I love about the Iron Man suit is that it's both an augmentation, and Tony Stark can drive it. And it's also an agent, and in some of the movies, the Iron Man suit is quite autonomous, and can fly around, and find Tony and all this kind of stuff. And so, this is the autonomy slider is, we can be, we can build augmentations, or we can build agents. And we kind of want to do a bit of both, but at this stage, I would say, working with fallible LLMs, and so on, I would say, you know, it's less Iron Man robots, and more Iron Man suits that you want to build. It's less like building flashy demos of autonomous agents, and more building partial autonomy products. And these products have custom GUIs and UI/UX. And we're trying to, um, and this is done so that the generation verification loop with the human is very, very fast. But we are not losing the sight of the fact that it is in principle possible to automate this work. And there should be an autonomy slider in your product. And you should be thinking about how you can slide that autonomy slider and make your product, uh, sort of more autonomous over time. But this is kind of how, I think there's lots of opportunities in these kinds of products. The other thing I really like is a number of little tools here and there that are helping ingest data that, in like, very LLM-friendly formats. So, for example, when I go to a GitHub repo, like my NanaGPT repo, I can't feed this to an LLM and ask questions about it, uh, because it's, you know, this is a human interface on GitHub. So, when you just change the URL from GitHub to GitIngest, then, uh, this will actually concatenate all the files into a single giant text. And it will create a directory structure, et cetera. And this is ready to be copy-pasted into your favorite LLM, and you can do stuff. Maybe even more a dramatic example of this is DeepWiki. Where it's not just the raw content of these files, Uh, this is from Devin. But also, like, they have Devin basically do analysis of the GitHub repo. And Devin basically builds up a whole docs, uh, pages just for your repo. And you can imagine that this is even more helpful to copy-paste into your LLM. And so, I love all the little tools that basically, where you just change the URL, and it makes something accessible to an LLM. So, this is all well and great. And, uh, yeah, I think there should be a lot more of it. One more note I wanted to make is that it is absolutely possible that in the future, LLMs will be able to, this is not even future, this is today. They'll be able to go around and they'll be able to click stuff and so on. But I still think it's very worth, uh, basically, meeting LLM halfway. LLMs halfway, and making it easier for them to access all this information. Uh, because this is still fairly expensive, I would say, to use, and, uh, a lot more difficult. And so, I do think that lots of software, there will be a long tail where it won't, uh, adapt because these are not like live player, sort of, repositories or digital infrastructure. And we will need these tools. Uh, but I think for everyone else, I think it's very worth, kind of, like, meeting in some middle point. So, I'm bullish on both, if that makes sense. So, in summary, what an amazing time to get into the industry. We need to rewrite a ton of code. A ton of code will be written by professionals and vibe coders. These LLMs are kind of like utilities, kind of like fabs, but they're kind of especially like operating systems. But it's so early. It's like 1960s of operating systems. And, uh, and I think a lot of the analogies cross over. Um, and these LLMs are kind of like these fallible, uh, you know, people spirits that we have to learn to work with. And in order to do that properly, we need to adjust our infrastructure towards it. So, when you're building these LLM apps, I described some of the ways of working effectively with these LLMs, and some of the tools that make that, uh, kind of, uh, possible. And how you can spin this loop very, very quickly. And, basically, create partial autonomy products. And then, um, yeah, a lot of code has to also be written for the agents, or directly. But, in any case, going back to the Iron Man suit analogy. I think what we'll see over the next decade, roughly, is we're going to take the slider from left to right. And I'm very interesting, it's going to be very interesting to see what that looks like. And I can't wait to build it with all of you. Thank you.
